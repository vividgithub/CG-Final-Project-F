{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Github\\pointnet-reprd\n",
      "D:\\Github\\pointnet-reprd\\models\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"D:\\Github\\pointnet-reprd\\train\\train.py\"))\n",
    "ROOT_DIR = 'D:\\Github\\pointnet-reprd'\n",
    "print(ROOT_DIR)\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "print(os.path.join(ROOT_DIR, 'models'))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
    "import provider\n",
    "import tf_util\n",
    "import modelnet_dataset\n",
    "import modelnet_h5_dataset\n",
    "from pointnet_util import sample_and_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS:\n",
    "    batch_size=16\n",
    "    num_point=1024\n",
    "    max_epoch=251\n",
    "    learning_rate=0.001\n",
    "    gpu=0\n",
    "    momentum=0.9\n",
    "    optimizer='adam'\n",
    "    decay_step=200000\n",
    "    decay_rate=0.7\n",
    "    log_dir='log'\n",
    "    normal=False\n",
    "    model='pointnet2_cls_ssg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_CNT = 0\n",
    "\n",
    "BATCH_SIZE = FLAGS.batch_size\n",
    "NUM_POINT = FLAGS.num_point\n",
    "MAX_EPOCH = FLAGS.max_epoch\n",
    "BASE_LEARNING_RATE = FLAGS.learning_rate\n",
    "GPU_INDEX = FLAGS.gpu\n",
    "MOMENTUM = FLAGS.momentum\n",
    "OPTIMIZER = FLAGS.optimizer\n",
    "DECAY_STEP = FLAGS.decay_step\n",
    "DECAY_RATE = FLAGS.decay_rate\n",
    "\n",
    "MODEL = importlib.import_module(FLAGS.model) # import network module\n",
    "#MODEL_FILE = os.path.join(ROOT_DIR, 'models', FLAGS.model+'.py')\n",
    "LOG_DIR = FLAGS.log_dir\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "#os.system('cp %s %s' % (MODEL_FILE, LOG_DIR)) # bkp of model def\n",
    "os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "\n",
    "# Shapenet official train/test split\n",
    "if FLAGS.normal:\n",
    "    assert(NUM_POINT<=10000)\n",
    "    DATA_PATH = os.path.join(ROOT_DIR, 'data/modelnet40_normal_resampled')\n",
    "    TRAIN_DATASET = modelnet_dataset.ModelNetDataset(root=DATA_PATH, npoints=NUM_POINT, split='train', normal_channel=FLAGS.normal, batch_size=BATCH_SIZE)\n",
    "    TEST_DATASET = modelnet_dataset.ModelNetDataset(root=DATA_PATH, npoints=NUM_POINT, split='test', normal_channel=FLAGS.normal, batch_size=BATCH_SIZE)\n",
    "else:\n",
    "    assert(NUM_POINT<=2048)\n",
    "    TRAIN_DATASET = modelnet_h5_dataset.ModelNetH5Dataset('D:/Github/pointnet-reprd/data/modelnet40_ply_hdf5_2048/train_files.txt', batch_size=BATCH_SIZE, npoints=NUM_POINT, shuffle=True)\n",
    "    TEST_DATASET = modelnet_h5_dataset.ModelNetH5Dataset( 'D:/Github/pointnet-reprd/data/modelnet40_ply_hdf5_2048/test_files.txt', batch_size=BATCH_SIZE, npoints=NUM_POINT, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(1):#MAX_EPOCH):\n",
    "        train_one_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "    \n",
    "    log_string(str(datetime.now()))\n",
    "\n",
    "    # Make sure batch data is of same size\n",
    "    cur_batch_data = np.zeros((BATCH_SIZE,NUM_POINT,TRAIN_DATASET.num_channel()))\n",
    "    cur_batch_label = np.zeros((BATCH_SIZE), dtype=np.int32)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    batch_idx = 0\n",
    "    \n",
    "    while TRAIN_DATASET.has_next_batch():\n",
    "        batch_data, batch_label = TRAIN_DATASET.next_batch(augment=True)\n",
    "        #batch_data = provider.random_point_dropout(batch_data)\n",
    "        bsize = batch_data.shape[0]\n",
    "        cur_batch_data[0:bsize,...] = batch_data\n",
    "        cur_batch_label[0:bsize] = batch_label\n",
    "        ####\n",
    "        is_training_pl=True\n",
    "        pointclouds_pl = cur_batch_data\n",
    "        labels_pl = cur_batch_label\n",
    "        #pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "        new_xyz, new_points, idx, grouped_xyz = sample_and_group(512, 0.2, 32, pointclouds_pl, None)\n",
    "        ######\n",
    "        total_seen += bsize\n",
    "\n",
    "        if (batch_idx+1)%50 == 0:\n",
    "            log_string(' ---- batch: %03d ----' % (batch_idx+1))\n",
    "\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "        batch_idx += 1\n",
    "        print(batch_label)\n",
    "    TRAIN_DATASET.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(point_cloud, is_training, bn_decay=None):\n",
    "    \"\"\" Classification PointNet, input is BxNx3, output Bx40 \"\"\"\n",
    "    batch_size = point_cloud.get_shape()[0].value\n",
    "    num_point = point_cloud.get_shape()[1].value\n",
    "    end_points = {}\n",
    "    l0_xyz = point_cloud\n",
    "    l0_points = None\n",
    "    end_points['l0_xyz'] = l0_xyz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Set abstraction layers\n",
    "    # Note: When using NCHW for layer 2, we see increased GPU memory usage (in TF1.4).\n",
    "    # So we only use NCHW for layer 1 until this issue can be resolved.\n",
    "    l1_xyz, l1_points, l1_indices = pointnet_sa_module(l0_xyz, l0_points, npoint=512, radius=0.2, nsample=32, mlp=[64,64,128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer1', use_nchw=True)\n",
    "    l2_xyz, l2_points, l2_indices = pointnet_sa_module(l1_xyz, l1_points, npoint=128, radius=0.4, nsample=64, mlp=[128,128,256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope='layer2')\n",
    "    l3_xyz, l3_points, l3_indices = pointnet_sa_module(l2_xyz, l2_points, npoint=None, radius=None, nsample=None, mlp=[256,512,1024], mlp2=None, group_all=True, is_training=is_training, bn_decay=bn_decay, scope='layer3')\n",
    "\n",
    "    # Fully connected layers\n",
    "    net = tf.reshape(l3_points, [batch_size, -1])\n",
    "    net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training, scope='fc1', bn_decay=bn_decay)\n",
    "    net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp1')\n",
    "    net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training, scope='fc2', bn_decay=bn_decay)\n",
    "    net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope='dp2')\n",
    "    net = tf_util.fully_connected(net, 40, activation_fn=None, scope='fc3')\n",
    "\n",
    "    return net, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid: 2108\n",
      "2019-12-04 12:54:38.739197\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-44484fc2523c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mlog_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pid: %s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mLOG_FOUT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-58650cf63647>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#MAX_EPOCH):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-58-c5ea65f0c2ff>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mlabels_pl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_batch_label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mnew_xyz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrouped_xyz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_and_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpointclouds_pl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;31m######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mtotal_seen\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbsize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Github\\pointnet-reprd\\pointnet_util.py\u001b[0m in \u001b[0;36msample_and_group\u001b[1;34m(npoint, radius, nsample, xyz, points, knn, use_xyz, returnfps)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \"\"\"\n\u001b[0;32m    160\u001b[0m     \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxyz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m     \u001b[0mfps_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfarthest_point_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxyz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpoint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# [B, npoint, C]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mnew_xyz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxyz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfps_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Github\\pointnet-reprd\\pointnet_util.py\u001b[0m in \u001b[0;36mfarthest_point_sample\u001b[1;34m(xyz, npoint)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mbatch_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mcentroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfarthest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mcentroid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxyz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfarthest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    log_string('pid: %s'%(str(os.getpid())))\n",
    "    train()\n",
    "    LOG_FOUT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
